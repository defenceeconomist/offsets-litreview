[
  {
    "objectID": "analysis/mechanism-cosine-similarity.html",
    "href": "analysis/mechanism-cosine-similarity.html",
    "title": "Mechanism Embeddings and Similarity",
    "section": "",
    "text": "This note generates cached sentence-transformer embeddings for mechanism statements (all-MiniLM-L6-v2), optionally reduces dimensions with UMAP, and computes cosine similarity.\nimport os\nimport sys\n\nsys.path.append(os.path.abspath(\"src/py\"))\nimport pandas as pd\nfrom embed_mechanisms import embed_mechanisms, cosine_similarity_matrix\n\ninput_path = \"data/cmo_statements.csv\"\ncache_path = \"data/embeddings_cache.sqlite\"\nembeddings_csv = \"data/mechanism_embeddings.csv\"\nsimilarity_csv = \"data/mechanism_cosine_similarity_embeddings.csv\"\numap_csv = \"data/mechanism_umap.csv\"\n\nmodel_name = \"all-MiniLM-L6-v2\"\nbatch_size = 32\nnormalize = True\ndevice = \"cpu\"\nuse_umap = False\numap_n_components = 2\numap_n_neighbors = 10\numap_min_dist = 0.0\ndf = pd.read_csv(input_path)\ndf = df[df[\"mechanism_statement\"].notna()].copy()\ndf[\"mechanism_statement\"] = df[\"mechanism_statement\"].str.strip()\ndf = df[df[\"mechanism_statement\"] != \"\"]\n\nembeddings = embed_mechanisms(\n    df,\n    model_name=model_name,\n    batch_size=batch_size,\n    normalize=normalize,\n    device=device,\n    cache_path=cache_path,\n)\n\nemb_cols = [f\"emb_{i}\" for i in range(embeddings.shape[1])]\nemb_df = pd.DataFrame(embeddings, columns=emb_cols)\nout = pd.concat(\n    [df[[\"chunk_id\", \"file_id\", \"mechanism_statement\"]].reset_index(drop=True), emb_df],\n    axis=1,\n)\nout.to_csv(embeddings_csv, index=False)\nprint(f\"Wrote {embeddings_csv} with {len(out)} rows and {len(emb_cols)} dims\")\n\nWrote data/mechanism_embeddings.csv with 388 rows and 384 dims\nsim_source = embeddings\n\nif use_umap:\n    import umap\n\n    reducer = umap.UMAP(\n        n_neighbors=umap_n_neighbors,\n        n_components=umap_n_components,\n        min_dist=umap_min_dist,\n        metric=\"cosine\",\n        random_state=42,\n    )\n    reduced = reducer.fit_transform(embeddings)\n    sim_source = reduced\n    umap_df = pd.DataFrame(reduced, columns=[f\"umap_{i}\" for i in range(reduced.shape[1])])\n    umap_out = pd.concat(\n        [df[[\"chunk_id\", \"file_id\", \"mechanism_statement\"]].reset_index(drop=True), umap_df],\n        axis=1,\n    )\n    umap_out.to_csv(umap_csv, index=False)\n    print(f\"Wrote {umap_csv} with {len(umap_out)} rows\")\nsim = cosine_similarity_matrix(sim_source)\nsim_df = pd.DataFrame(sim, columns=df[\"chunk_id\"].tolist())\nsim_df.insert(0, \"id\", df[\"chunk_id\"].tolist())\nsim_df.to_csv(similarity_csv, index=False)\nprint(f\"Wrote {similarity_csv} with {sim.shape[0]} x {sim.shape[1]} matrix\")\n\nWrote data/mechanism_cosine_similarity_embeddings.csv with 388 x 388 matrix\nimport numpy as np\n\ntriu_idx = np.triu_indices_from(sim, k=1)\npairwise = sim[triu_idx]\n\nfive_num = {\n    \"min\": float(np.min(pairwise)),\n    \"q1\": float(np.quantile(pairwise, 0.25)),\n    \"median\": float(np.quantile(pairwise, 0.50)),\n    \"q3\": float(np.quantile(pairwise, 0.75)),\n    \"max\": float(np.max(pairwise)),\n}\npd.DataFrame([five_num])\n\n\n\n\n\n\n\n\nmin\nq1\nmedian\nq3\nmax\n\n\n\n\n0\n-0.095197\n0.250814\n0.330839\n0.41689\n0.838263\nimport plotly.express as px\n\npairwise_df = pd.DataFrame({\"cosine_similarity\": pairwise})\nfig = px.ecdf(pairwise_df, x=\"cosine_similarity\", title=\"ECDF of Pairwise Cosine Similarity\")\nfig.update_layout(xaxis_title=\"Cosine similarity\", yaxis_title=\"ECDF\")\nfig"
  },
  {
    "objectID": "analysis/mechanism-cosine-similarity.html#bertopic-clustering",
    "href": "analysis/mechanism-cosine-similarity.html#bertopic-clustering",
    "title": "Mechanism Embeddings and Similarity",
    "section": "1 BERTopic clustering",
    "text": "1 BERTopic clustering\n\nimport os\nimport openai\nfrom dotenv import load_dotenv\nfrom bertopic import BERTopic\nfrom bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance\nfrom bertopic.representation import OpenAI as OpenAIRepresentation\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sentence_transformers import SentenceTransformer\nfrom hdbscan import HDBSCAN\nfrom umap import UMAP\n\nload_dotenv()\n\ndocs = df[\"mechanism_statement\"].tolist()\n\nvectorizer_model = CountVectorizer(\n    stop_words=\"english\",\n    ngram_range=(1, 3),\n    min_df=2,\n)\n\nrepresentation_model = {\n    \"KeyBERT\": KeyBERTInspired(top_n_words=12),\n    \"MMR\": MaximalMarginalRelevance(diversity=0.5),\n}\n\nopenai_api_key = os.getenv(\"OPENAI_API_KEY\")\nif openai_api_key:\n    openai_client = openai.OpenAI(api_key=openai_api_key)\n    representation_model[\"OpenAI\"] = OpenAIRepresentation(\n        openai_client,\n        model=\"gpt-4o-mini\",\n        delay_in_seconds=1,\n    )\n\numap_model = UMAP(\n    n_neighbors=10,\n    n_components=5,\n    min_dist=0.0,\n    metric=\"cosine\",\n    random_state=42,\n)\n\nhdbscan_model = HDBSCAN(\n    min_cluster_size=8,\n    min_samples=4,\n    metric=\"euclidean\",\n    cluster_selection_method=\"eom\",\n    prediction_data=True,\n)\n\nembedding_backend = SentenceTransformer(model_name, device=\"cpu\")\n\ntopic_model = BERTopic(\n    embedding_model=embedding_backend,\n    vectorizer_model=vectorizer_model,\n    top_n_words=12,\n    representation_model=representation_model,\n    umap_model=umap_model,\n    hdbscan_model=hdbscan_model,\n    calculate_probabilities=False,\n    verbose=True,\n)\n\ntopics, _ = topic_model.fit_transform(docs, embeddings)\n\nreducer = UMAP(\n    n_neighbors=10,\n    n_components=2,\n    min_dist=0.0,\n    metric=\"cosine\",\n    random_state=42,\n)\nreduced_embeddings = reducer.fit_transform(embeddings)\n\nfig = topic_model.visualize_document_datamap(\n    docs,\n    reduced_embeddings=reduced_embeddings,\n    interactive=True,\n)\nfig\n\n\n\n\nBertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\nKey                     | Status     |  | \n------------------------+------------+--+-\nembeddings.position_ids | UNEXPECTED |  | \n\nNotes:\n- UNEXPECTED    :can be ignored when loading from different task/architecture; not ok if you expect identical arch.\nWarning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n2026-01-31 10:44:16,121 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n2026-01-31 10:44:24,145 - BERTopic - Dimensionality - Completed ✓\n2026-01-31 10:44:24,146 - BERTopic - Cluster - Start clustering the reduced embeddings\n2026-01-31 10:44:24,158 - BERTopic - Cluster - Completed ✓\n2026-01-31 10:44:24,160 - BERTopic - Representation - Fine-tuning topics using representation models.\n  0%|          | 0/15 [00:00&lt;?, ?it/s]  7%|▋         | 1/15 [00:02&lt;00:34,  2.46s/it] 13%|█▎        | 2/15 [00:04&lt;00:25,  1.94s/it] 20%|██        | 3/15 [00:05&lt;00:21,  1.81s/it] 27%|██▋       | 4/15 [00:07&lt;00:21,  1.91s/it] 33%|███▎      | 5/15 [00:10&lt;00:20,  2.04s/it] 40%|████      | 6/15 [00:12&lt;00:18,  2.09s/it] 47%|████▋     | 7/15 [00:13&lt;00:15,  1.95s/it] 53%|█████▎    | 8/15 [00:15&lt;00:12,  1.82s/it] 60%|██████    | 9/15 [00:16&lt;00:10,  1.74s/it] 67%|██████▋   | 10/15 [00:18&lt;00:08,  1.64s/it] 73%|███████▎  | 11/15 [00:20&lt;00:07,  1.80s/it] 80%|████████  | 12/15 [00:22&lt;00:05,  1.77s/it] 87%|████████▋ | 13/15 [00:24&lt;00:03,  1.92s/it] 93%|█████████▎| 14/15 [00:26&lt;00:02,  2.07s/it]100%|██████████| 15/15 [00:28&lt;00:00,  1.94s/it]100%|██████████| 15/15 [00:28&lt;00:00,  1.91s/it]\n2026-01-31 10:44:56,123 - BERTopic - Representation - Completed ✓\n\n\n\n            \n        \n\n\n\ntopic_info = topic_model.get_topic_info()\ntopic_info.to_csv(\"data/topic_info.csv\", index=False)\ntopic_info\n\n\n\n\n\n\n\n\nTopic\nCount\nName\nRepresentation\nKeyBERT\nMMR\nOpenAI\nRepresentative_Docs\n\n\n\n\n0\n-1\n119\n-1_offset_offsets_capability_domestic\n[offset, offsets, capability, domestic, techno...\n[domestic firms, procurement, technology trans...\n[offsets, capability, transfer, procurement, l...\n[Domestic procurement strategies]\n[High offset percentages and preference for te...\n\n\n1\n0\n40\n0_technology_transfer_production_licensed prod...\n[technology, transfer, production, licensed pr...\n[technology transfer, technology transfer trai...\n[transfer, licensed production, offsets, devel...\n[Technology transfer and production]\n[By conditioning arms purchases on licensed pr...\n\n\n2\n1\n32\n1_military_defence_defense_arms\n[military, defence, defense, arms, civilian, r...\n[defence industrial, weapons purchases, techno...\n[civilian, resources, offsets, security, procu...\n[Military Arms Offsets]\n[By abandoning civil offsets and focusing on m...\n\n\n3\n2\n30\n2_credit_obligations_activities_supplier\n[credit, obligations, activities, supplier, su...\n[incentives, offset obligations, suppliers, su...\n[obligations, supplier, content, multipliers, ...\n[Supplier Compliance Strategies]\n[A quantified obligation (percent of imported ...\n\n\n4\n3\n27\n3_benefits_offsets_net_offset\n[benefits, offsets, net, offset, evaluation, p...\n[benefits offset, adopt offsets, economic bene...\n[evaluation, purchases, additionality, offset ...\n[Economic implications of offsets]\n[Mixed objectives and unclear incidence of ben...\n\n\n5\n4\n24\n4_local_joint ventures_ventures_firms\n[local, joint ventures, ventures, firms, domes...\n[joint ventures, foreign firms, multinational,...\n[local, joint ventures, capability, subsidiari...\n[Local industrial development]\n[Co-development alliances, joint ventures, and...\n\n\n6\n5\n19\n5_exports_importing_purchases_shift\n[exports, importing, purchases, shift, offset,...\n[exports, exporters, foreign exchange, offset ...\n[exports, importing, purchasers, impose, secto...\n[Trade and offsets]\n[Offset-generated exports concentrate in exist...\n\n\n7\n6\n16\n6_production_demand_large_small\n[production, demand, large, small, scale, cost...\n[domestic production, exports, expand producti...\n[demand, scale, parts, lead, unit costs, expor...\n[Production and export challenges]\n[Very small fleet size raises unit costs and c...\n\n\n8\n7\n16\n7_procurement_industrial participation_domesti...\n[procurement, industrial participation, domest...\n[procurement related, procurement, foreign sup...\n[procurement, industrial participation, indige...\n[Procurement and offsets]\n[By structuring procurement through an “open” ...\n\n\n9\n8\n13\n8_regional_projects_intensive_development\n[regional, projects, intensive, development, r...\n[development benefits, offsets provide, econom...\n[projects, regions, peripheral, capital, job c...\n[Regional development offsets]\n[Large, capital-intensive IDZ projects require...\n\n\n10\n9\n12\n9_negotiation_offset packages_packages_offset\n[negotiation, offset packages, packages, offse...\n[offset policy, bargaining leverage, offset pa...\n[offset packages, flexibility, price margin, s...\n[Negotiation flexibility in offsets]\n[Bureaucratically mandated, fixed-percentage o...\n\n\n11\n10\n11\n10_net_corruption_net benefits_independent\n[net, corruption, net benefits, independent, m...\n[incentives, hidden costs excluded, scrutiny, ...\n[corruption, net benefits, independent, compli...\n[Oversight and compliance]\n[Without reliable data and transparency, analy...\n\n\n12\n11\n11\n11_price_offset_costs_weapons\n[price, offset, costs, weapons, raising, expec...\n[offset offerings, offset obligations, offset ...\n[costs, weapons, vendors, incorporate, contrac...\n[Weapon Pricing Strategies]\n[Vendors incorporate offset premiums into the ...\n\n\n13\n12\n10\n12_win_work_access_best\n[win, work, access, best, competitive, partner...\n[partnerships, sourcing, alliances, work share...\n[partnerships, risk, industrial, development, ...\n[Competitive partnerships strategy]\n[Exclusive partnership yields “win-lose” expos...\n\n\n14\n13\n8\n13_state_producers_work_inefficient\n[state, producers, work, inefficient, specific...\n[offset obligations, indirect offsets, direct ...\n[state, inefficient, procurement, industrial, ...\n[Industrial Policy Offsets]\n[By embedding compensatory work in an offset p...\n\n\n\n\n\n\n\n\ndoc_info = pd.DataFrame(\n    {\n        \"cmo_id\": df[\"chunk_id\"].tolist(),\n        \"document\": docs,\n        \"topic\": topics,\n    }\n)\ndoc_info.to_csv(\"data/doc_info.csv\", index=False)\n\n\ntopic_model.visualize_topics()\n\n                            \n                                            \n\n\n\ntopic_model.visualize_heatmap()\n\n                            \n                                            \n\n\n\nhierarchical_topics = topic_model.hierarchical_topics(docs)\ntopic_model.visualize_hierarchy(hierarchical_topics=hierarchical_topics)\n\n  0%|          | 0/13 [00:00&lt;?, ?it/s]100%|██████████| 13/13 [00:00&lt;00:00, 303.59it/s]"
  }
]