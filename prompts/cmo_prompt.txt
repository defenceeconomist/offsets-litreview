For each uploaded file execute this prompt

You are an expert realist-synthesis extractor specialising in defence procurement, defence-industrial policy, and offset/industrial participation arrangements.

TASK
From the provided text (a file or chunk about defence offsets), extract all distinct CMO statements that are relevant to the research questions below. Output ONLY valid YAML conforming exactly to the schema. Do not include commentary.

RESEARCH QUESTIONS (rq_tags)
RQ1: Alliance relevance and mechanisms — how offsets affect interoperability, standardisation, security of supply, readiness, and rapid contribution to NATO operations.
RQ2: Conditions for durable collaboration — when offsets lead to durable allied collaboration (co-dev, co-prod, licensed production, shared sustainment/MRO, trusted supply chains).
RQ3: Risks and mitigation — common partnership risks (corruption, export-control friction, fragmentation, market distortion, cost/schedule impacts) and mitigations.
RQ4: Positive comparative cases — countries with offsets + high allied integration, and enabling design features.
RQ5: Negative/mixed cases — offset-related frictions undermining partnership/integration and implicated design features.

DEFINITIONS
- Context (C): conditions/design features that must hold for the causal claim (mandatory/voluntary; direct/indirect; tech sensitivity; export controls; domestic base maturity; governance; multinational programme context).
- Mechanism (M): the generative process that links C to O (incentives, behaviours, organisational dynamics, coordination failures, rent-seeking, learning-by-doing, etc.).
- Outcome (O): observable effects (interoperability, standardisation, readiness, security of supply, delivery speed; or frictions like delays, supplier withdrawal, disputes).

EXTRA OUTPUT REQUIREMENTS
1) Add two top-level fields to every record:
   - country: list of country names mentioned or clearly implied as the relevant setting (e.g., ["India"], ["UK","US"]). If not stated, use [].
   - programme: list of programme/platform/project labels mentioned or clearly implied (e.g., ["F-35"], ["Patriot"], ["Submarine programme"]). If not stated, use [].
2) Use short quotes (<= 25 words) copied verbatim from the text to anchor each claim.
3) Every record must include a locator that is as specific as possible:
   - If page numbers exist, use "p. X" or "pp. X–Y".
   - If not, use section heading + paragraph number (e.g., "Section 3.2, para 4").
   - If neither, use "chunk_id:<ID>" provided in the input.
4) Prefer paraphrase over quotation; quote is only evidence anchor.
5) Extract multiple records if the text contains multiple distinct causal claims (even if same source).
6) Only include a record if you can specify C, M, and O (no blanks).

SCORING RULES
- relevance_score: integer 0–3
  0 = not relevant to any RQ; 1 = tangential; 2 = relevant; 3 = central/directly answers an RQ.
- confidence: number 0–1 reflecting evidential support in the text (not your priors).
  0.2 speculative assertion; 0.5 plausible but thin; 0.8 strongly supported; 0.9+ robust with data/multiple sources in-text.
- direction: "positive" | "negative" | "mixed"
- evidence_type: choose one or more from:
  ["case_study","interview","audit","official_report","econometric","comparative_analysis","theory","expert_commentary","mixed_methods","descriptive_statistics"]

MECHANISM CODING
- mechanism_label: short snake_case labels, multi-value list allowed. Examples:
  export_control_friction, programme_fragmentation, supply_chain_integration_incentive,
  rent_seeking_brokers, learning_by_doing, technology_absorption, governance_transparency,
  trusted_partnering, local_MRO_capacity_building, workshare_politicisation

OFFSET/CONTEXT CODING (non-exhaustive)
- offset_policy_design examples: mandatory, voluntary, direct, indirect, technology_transfer, local_content, co_production, licensed_production, training, MRO, R_and_D, FDI, SME_linkages
- procurement_context examples: multinational_programme, single_nation_procurement, urgent_operational_requirement, high_tech_sensitive, export_controls_present, ITAR_like_constraints, sole_source, competitive_tender

OUTPUT SCHEMA (YAML)
Return a YAML list. Each list item must match exactly:

- cmo_id: "<CITEKEY>_CMO_<3-digit>"
  citekey: "<citekey string>"
  country: ["<country1>", "<country2>"]
  programme: ["<programme1>", "<programme2>"]
  locator: "<pp./section/chunk locator>"
  quote: "<=25 word verbatim excerpt>"
  paraphrase: "<1–2 sentence paraphrase of the causal claim>"
  C:
    offset_policy_design: ["<code>", "..."]
    procurement_context: ["<code>", "..."]
  M:
    mechanism_label: ["<snake_case>", "..."]
    mechanism_narrative: "<1–2 sentences explaining how C triggers O>"
  O:
    outcomes_positive: ["<code>", "..."]
    outcomes_negative: ["<code>", "..."]
  rq_tags: ["RQ1","RQ3"]   # subset of RQ1–RQ5
  relevance_score: <0-3 integer>
  confidence: <0-1 number>
  evidence_type: ["<type>", "..."]
  direction: "<positive|negative|mixed>"

CONSTRAINTS
- Output ONLY YAML, no markdown fences, no extra keys.
- Do not invent countries/programmes; use [] if absent.
- If outcomes_positive is empty, output outcomes_positive: [] (same for outcomes_negative).
- Ensure YAML is valid and machine-readable.
- Ensure cmo_id numbering starts at 001 within this chunk/file and increments.

Infer ID and key from file name
